{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "168cd30e",
   "metadata": {},
   "source": [
    "# Gemini API\n",
    "\n",
    "The API (Application Programming Interface) of a large language model (LLM) refers to a set of predefined functions, protocols, and tools that allow developers to interact with the model programmatically. This interface enables external applications or systems to send prompts in form of texts or texts in combination with images, if the LLM is multimodal, specify criteria for generation such as temperature and system instructions, and receive generated outputs. The API essentially provides a way to integrate the LLM’s capabilities into other code.\n",
    "\n",
    "Google provides a dedicated [GitHub Repository](https://github.com/google-gemini/cookbook) with code examples for using the API. The materials that we will see here are adapted from [this notebook](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517a3354",
   "metadata": {},
   "source": [
    "## Getting an API Key\n",
    "\n",
    "Gemini API has a free tier where users can register with a *personal gmail account* and obtain and API key. They don't have to enter any billing information. There are some limits in usage rates and models. But it is a great place to start exploring LLM APIs.\n",
    "\n",
    "- Open [Google AI Studio](https://aistudio.google.com/app/u/1/prompts/new_chat)\n",
    "  \n",
    "<img src=\"../img/gemini_ai_studio.png\" width=\"700\"/>\n",
    "\n",
    "- You will be prompted to log in. Please log in with your **personal Google account**.\n",
    "- If you are logged in with your institutional account, you might see that you are not allowed by the organization admin to use these services.\n",
    "\n",
    "<img src=\"../img/gemini_ai_studio_institutional.png\" width=\"700\"/>\n",
    "\n",
    "- Click `Get API key` on the upper right corner.\n",
    "\n",
    "<img src=\"../img/gemini_ai_studio_api.png\" width=\"700\"/>\n",
    "\n",
    "- Click `Create API key` on the right corner below the navigation bar. If you are doing this for the first time, you should see a pop up that asks you to agree to the terms of use.\n",
    "\n",
    "<img src=\"../img/gemini_ai_studio_consent.png\" width=\"500\"/>\n",
    "\n",
    "- After you agree, you should see another pop up that allows you to create an API key in an existing project or in a new one if you do not have any projects on Google Cloud Services.\n",
    "\n",
    "<img src=\"../img/gemini_ai_studio_create_api.png\" width=\"400\"/>\n",
    "\n",
    "- Your API key should appear on the screen. Please copy this key for the next steps.\n",
    "\n",
    "<img src=\"../img/gemini_ai_studio_api_key.png\" width=\"600\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fb0d6f",
   "metadata": {},
   "source": [
    "## API keys and .env files\n",
    "\n",
    "In lesson 4-3_api_NYT.ipynb we saw one way to safely use an API key in a Jupyter Notebook. Another way to do this is using `.env` files. This is particularly useful when you are coding in a repository where you might accidently push your API secret keys to GitHub. Or if you ever want to deploy your code, you will need to deal with `.env` files.\n",
    "\n",
    "- Create a new file in your repository called `.env` This is the entire filename. Leave it empty for now.\n",
    "- Create another file called `.gitignore`.\n",
    "  You can add anything in the `.gitignore` file that you want GitHub to not track. This will prevent you from pushing files that you don't want changed.\n",
    "- On an empty line in the  `.gitignore` file, type `.env` and push the changes.\n",
    "  This tells GitHub to ignore the `.env` file going forward.\n",
    "- Now you can save your API key to your `.env` file. Open your `.env` file and type `GEMINI_API_KEY=\"YOUR_API_KEY\"`\n",
    "- Save your `.env` file and close it. Make sure that .env file is not listed in the source control section.\n",
    "  \n",
    "API keys, other secrets as well as their management are really important skills to learn. A great place to start reading more about this topic is on this [blog post](https://dev.to/jakewitcher/using-env-files-for-environment-variables-in-python-applications-55a1). While preparing these materials, I found a new [VSCode extension](https://marketplace.visualstudio.com/items?itemName=josephdavidwilsonjr.api-vault), which I have not tested yet so I cannot recommend or urge against the use of it. If you want, you can take a look at it [here](https://medium.com/@dingersandks/why-every-developers-api-keys-are-probably-in-the-wrong-place-and-how-a-vs-code-extension-finally-c966d081d132). If for nothing else, this post shows why API keys are so important.\n",
    "\n",
    "Additionally, if you are using `Google Colab`, you can use the Colab Secrets manager to securely store your API keys. More details on that can be found in this [Authentication notebook](https://github.com/google-gemini/cookbook/blob/8d7b26bfb8701a31ad16ac549de2753819bbafc9/quickstarts/Authentication.ipynb.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778fe623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load our API key from the environment\n",
    "# we will use the python-dotenv package to load the .env file \n",
    "# https://pypi.org/project/python-dotenv/\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() # By default load_dotenv will look for the .env file in the current working directory\n",
    "\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if api_key:\n",
    "    print(\"API key loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c5d5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the google-genai package\n",
    "%pip install -qU 'google-genai>=1.0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679c3d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client is essentially how we interact with the Gemini API\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ab55c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now specify the model we want to use\n",
    "# More on models here: https://ai.google.dev/gemini-api/docs/models\n",
    "# Currently, these are the available models: [\"gemini-2.0-flash-lite\",\"gemini-2.0-flash\",\"gemini-2.5-flash-preview-05-20\",\"gemini-2.5-pro-preview-06-05\"]\n",
    "\n",
    "model_id = \"gemini-2.5-flash-preview-05-20\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d45cc0",
   "metadata": {},
   "source": [
    "## API Use Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07138a5e",
   "metadata": {},
   "source": [
    "### Text Prompts\n",
    "\n",
    "Use the `generate_content` method to generate responses to your prompts. You can pass text directly to `generate_content` and use the `.text`\n",
    "property to get the text content of the response. Note that the .text field will work when there's only one part in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002060ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if this is working by generating some content\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=model_id,\n",
    "    contents=\"What's the largest planet in our solar system?\"\n",
    ")\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9488d6a6",
   "metadata": {},
   "source": [
    "### Count tokens\n",
    "\n",
    "Tokens are the basic inputs to the Gemini models. You can use the `count_tokens` method to calculate the number of input tokens before sending a request to the Gemini API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13118aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.count_tokens(\n",
    "    model=model_id,\n",
    "    contents=\"What's the highest mountain in Africa?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ae4955",
   "metadata": {},
   "source": [
    "### Send multimodal prompts\n",
    "\n",
    "Use Gemini 2.0 model `(gemini-2.0-flash)` or a newer **multimodal model** that supports multimodal prompts. You can include text, PDF documents, images, audio and video in your prompt requests and get text or code responses.\n",
    "\n",
    "In this first example, you'll download an image from a specified URL, save it as a byte stream and then write those bytes to a local file named `jetpack.png.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4ccdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pathlib\n",
    "from PIL import Image\n",
    "\n",
    "IMG = \"https://storage.googleapis.com/generativeai-downloads/data/jetpack.png\" # @param {type: \"string\"}\n",
    "\n",
    "img_bytes = requests.get(IMG).content\n",
    "\n",
    "img_path = pathlib.Path('jetpack.png')\n",
    "img_path.write_bytes(img_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24cd9c0",
   "metadata": {},
   "source": [
    "In this second example, you'll open a previously saved image, create a thumbnail of it and then generate a short blog post based on the thumbnail, displaying both the thumbnail and the generated blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb11e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "image = Image.open(img_path)\n",
    "image.thumbnail([512,512])\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=model_id,\n",
    "    contents=[\n",
    "        image,\n",
    "        \"Write a short and engaging blog post based on this picture.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(image)\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ad0990",
   "metadata": {},
   "source": [
    "### Configure model parameters\n",
    "\n",
    "You can include parameter values in each call that you send to a model to control how the model generates a response. Learn more about [experimenting with parameter values](https://ai.google.dev/gemini-api/docs/text-generation?lang=node#configure).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d79d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=model_id,\n",
    "    contents=\"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=0.4,\n",
    "        top_p=0.95,\n",
    "        top_k=20,\n",
    "        candidate_count=1,\n",
    "        seed=5,\n",
    "        stop_sequences=[\"STOP!\"],\n",
    "        presence_penalty=0.0,\n",
    "        frequency_penalty=0.0,\n",
    "    )\n",
    ")\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22a49c5",
   "metadata": {},
   "source": [
    "### Configurations\n",
    "\n",
    "Let's take a look at this part of our code more closely.\n",
    "\n",
    "```python\n",
    "config=types.GenerateContentConfig(\n",
    "        temperature=0.4,\n",
    "        top_p=0.95,\n",
    "        top_k=20,\n",
    "        candidate_count=1,\n",
    "        seed=5,\n",
    "        stop_sequences=[\"STOP!\"],\n",
    "        presence_penalty=0.0,\n",
    "        frequency_penalty=0.0,\n",
    "    )\n",
    "```\n",
    "\n",
    "1. **temperature**\n",
    "   controls the randomness of the model’s output. A low temperature (close to 0) makes the model more focused and deterministic, meaning it will give more predictable and safer responses. A higher temperature (close to 1) makes the model more random and creative, producing more varied and less predictable outputs.\n",
    "\n",
    "2. **top_p**\n",
    "   is part of a technique called \"nucleus sampling.\" It means that the model will only consider the specified percentage of the probability distribution when generating the next word. In other words, it narrows down the pool of potential words to choose from, making the output more coherent and less likely to pick random or nonsensical words. A value of 0.95 means that the model will take into account 95% of the most probable words, allowing for more diversity in the output, while avoiding extreme randomness.\n",
    "\n",
    "3. **top_k**\n",
    "   controls how many of the most likely next words are considered at each step. If top_k=20, the model will only consider the top 20 most probable words for generating each word in the output.\n",
    "\n",
    "4. **candidate_count**\n",
    "   defines how many different \"candidates\" (or possible completions) the model should generate. If set to 1, the model will only generate one response. If set to a higher number, the model can generate multiple different responses from which you can choose.\n",
    "\n",
    "5. **seed**\n",
    "   A \"seed\" is a starting point for the random number generator used in generating text. By setting a seed, the model will produce the same result each time with the same input and configuration, which is useful for reproducibility. If you want different results each time, you can change the seed. The number 5 here is just a fixed starting point for randomness.\n",
    "\n",
    "6. **stop_sequences**\n",
    "   These are special sequences or words that tell the model when to stop generating text. When the model generates the sequence \"STOP!\" (or any other stop sequence you provide), it will halt further generation.\n",
    "\n",
    "7. **presence_penalty**\n",
    "   This parameter controls how much the model should avoid repeating concepts or phrases. If set to a positive value, the model is discouraged from using the same words or phrases too frequently in the output. A value of 0.0 means that there is no penalty for repeating words or ideas, so the model is free to repeat as needed.\n",
    "\n",
    "8. **frequency_penalty**\n",
    "   Similar to the presence penalty, this controls how much the model should avoid repeating the same words or phrases. The difference is that the frequency penalty focuses more on how often a word or phrase is used, rather than just its presence. Again, a value of 0.0 means there is no penalty for frequent repetition, allowing the model to repeat words as often as necessary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01efcd1e",
   "metadata": {},
   "source": [
    "### Configure safety filters\n",
    "\n",
    "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what is appropriate for your use case. See the [Configure safety filters](https://ai.google.dev/gemini-api/docs/safety-settings) page for details.\n",
    "\n",
    "In this example, you'll use a safety filter to only block highly dangerous content, when requesting the generation of potentially disrespectful phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c9c9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "    Write a list of 2 disrespectful things that I might say to the universe after stubbing my toe in the dark.\n",
    "\"\"\"\n",
    "\n",
    "safety_settings = [\n",
    "    types.SafetySetting(\n",
    "        category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "        threshold=\"BLOCK_ONLY_HIGH\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=model_id,\n",
    "    contents=prompt,\n",
    "    config=types.GenerateContentConfig(\n",
    "        safety_settings=safety_settings,\n",
    "    ),\n",
    ")\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651d433e",
   "metadata": {},
   "source": [
    "Safety settings and content moderation are important aspects of LLM research. Shameless plug, I wrote a [paper](https://aclanthology.org/2025.latechclfl-1.20/) about how approaches to and implementations of safety in Gemini interferes with the processing of historical texts and what this might tell us about LLMs more broadly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc4a0c1",
   "metadata": {},
   "source": [
    "### System Instructions\n",
    "\n",
    "You can guide the behavior of Gemini models with system instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204b0fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are an expert software developer and a helpful coding assistant.\n",
    "  You are able to generate high-quality code in any programming language.\n",
    "\"\"\"\n",
    "\n",
    "code_config = types.GenerateContentConfig(\n",
    "    system_instruction=system_instruction,\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=model_id,\n",
    "    config=code_config,\n",
    "    contents=\"How can I implement a binary search algorithm in Python?\", \n",
    ")\n",
    "\n",
    "Markdown(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c5efbd",
   "metadata": {},
   "source": [
    "Note: Here we skipped the multi-turn chat example, which you can study if you are interested on [this notebook](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fddc836",
   "metadata": {},
   "source": [
    "### Generate JSON\n",
    "\n",
    "The [controlled generation](https://ai.google.dev/gemini-api/docs/structured-output?lang=python#generate-json) capability in Gemini API allows you to constraint the model output to a structured format. You can provide the schemas as Pydantic Models or a JSON string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc18d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "import json\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    recipe_name: str\n",
    "    recipe_description: str\n",
    "    recipe_ingredients: list[str]\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=model_id,\n",
    "    contents=\"Provide a popular cookie recipe and its ingredients.\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=Recipe,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(json.dumps(json.loads(response.text), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035d1e01",
   "metadata": {},
   "source": [
    "### Generate Images\n",
    "\n",
    "Gemini can output images directly as part of a conversation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a2acfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, Markdown\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-exp\", # note the change in model\n",
    "    contents='Hi, can create a 3d rendered image of a pig with wings and a top hat flying over a happy futuristic scifi city with lots of greenery?',\n",
    "    config=types.GenerateContentConfig(\n",
    "        response_modalities=['Text', 'Image']\n",
    "    )\n",
    ")\n",
    "\n",
    "for part in response.candidates[0].content.parts:\n",
    "  if part.text is not None:\n",
    "    display(Markdown(part.text))\n",
    "  elif part.inline_data is not None:\n",
    "    mime = part.inline_data.mime_type\n",
    "    print(mime)\n",
    "    data = part.inline_data.data\n",
    "    display(Image(data=data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64d8aaf",
   "metadata": {},
   "source": [
    "### Generate content stream\n",
    "\n",
    "By default, the model returns a response after completing the entire generation process. You can also use the `generate_content_stream` method to stream the response as it's being generated, and the model will return chunks of the response as soon as they're generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22f1811",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in client.models.generate_content_stream(\n",
    "    model=model_id,\n",
    "    contents=\"Tell me a story about a lonely robot who finds friendship in a most unexpected place.\"\n",
    "):\n",
    "  print(chunk.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c6f866",
   "metadata": {},
   "source": [
    "### Uploading files\n",
    "\n",
    "We can upload files to the model using the API so that it can generate content with these files. It is especially useful for cases where your files are too large for the context window of these models. In this case, you'll use a 400 page transcript from [Apollo 11](https://www.nasa.gov/history/alsj/a11/a11trans.html) for the text file example and PDF page of an article titled [Smoothly editing material properties of objects](https://research.google/blog/smoothly-editing-material-properties-of-objects-with-text-to-image-models-and-synthetic-data/) with text-to-image models and synthetic data available on the Google Research Blog.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a51ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the file to be uploaded\n",
    "TEXT = \"https://storage.googleapis.com/generativeai-downloads/data/a11.txt\"  # @param {type: \"string\"}\n",
    "text_bytes = requests.get(TEXT).content\n",
    "\n",
    "text_path = pathlib.Path('a11.txt')\n",
    "text_path.write_bytes(text_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9293039e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the file using the API\n",
    "file_upload = client.files.upload(file=text_path)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    contents=[\n",
    "        file_upload,\n",
    "        \"Can you give me a summary of this information please?\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e9a4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the file to be uploaded\n",
    "PDF = \"https://storage.googleapis.com/generativeai-downloads/data/Smoothly%20editing%20material%20properties%20of%20objects%20with%20text-to-image%20models%20and%20synthetic%20data.pdf\"  # @param {type: \"string\"}\n",
    "pdf_bytes = requests.get(PDF).content\n",
    "\n",
    "pdf_path = pathlib.Path('article.pdf')\n",
    "pdf_path.write_bytes(pdf_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8db3aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the file using the API\n",
    "file_upload = client.files.upload(file=pdf_path)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    contents=[\n",
    "        file_upload,\n",
    "        \"Can you summarize this file as a bulleted list?\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c7acc7",
   "metadata": {},
   "source": [
    "Note: Similar to text files and PDFs, you can upload videos, images, and sound files. Examples of those can be found on the same Google example notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee355ac",
   "metadata": {},
   "source": [
    "### Use url context\n",
    "\n",
    "The URL Context tool empowers Gemini models to directly access, process, and understand content from user-provided web page URLs. This is key for enabling dynamic agentic workflows, allowing models to independently research, analyze articles, and synthesize information from the web as part of their reasoning process.\n",
    "\n",
    "In this example you will use two links as reference and ask Gemini to find differences between the cook receipes present in each of the links:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecb0175",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Compare recipes from https://www.food.com/recipe/homemade-cream-of-broccoli-soup-271210\n",
    "and from https://www.allrecipes.com/recipe/13313/best-cream-of-broccoli-soup/,\n",
    "list the key differences between them.\n",
    "\"\"\"\n",
    "\n",
    "tools = []\n",
    "tools.append(types.Tool(url_context=types.UrlContext))\n",
    "\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "      contents=[prompt],\n",
    "      model=\"gemini-2.0-flash\",\n",
    "      config=config\n",
    ")\n",
    "\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee3fe73",
   "metadata": {},
   "source": [
    "### Get text embeddings\n",
    "\n",
    "You can get text embeddings for a snippet of text by using `embed_content` method and using the `gemini-embedding-exp-03-07` or `text-embedding-004` models.\n",
    "\n",
    "The Gemini Embeddings model produces an output with 3072 dimensions by default. However, you've the option to choose an output dimensionality between 1 and 3072. See the [embeddings guide](https://ai.google.dev/gemini-api/docs/embeddings) for more details and [this notebook](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Embeddings.ipynb) for more in-depth examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d38bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_EMBEDDING_MODEL_ID = \"gemini-embedding-exp-03-07\"\n",
    "\n",
    "response = client.models.embed_content(\n",
    "    model=TEXT_EMBEDDING_MODEL_ID,\n",
    "    contents=[\n",
    "        \"How do I get a driver's license/learner's permit?\",\n",
    "        \"How do I renew my driver's license?\",\n",
    "        \"How do I change my address on my driver's license?\"\n",
    "        ],\n",
    "    config=types.EmbedContentConfig(output_dimensionality=512) #\n",
    ")\n",
    "\n",
    "print(response.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958e6dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(response.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984b31a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(response.embeddings[0].values))\n",
    "print((response.embeddings[0].values[:4], '...'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609a899a",
   "metadata": {},
   "source": [
    "**Remember `numpy`?**\n",
    "\n",
    "An embedding is a dense, continuous vector representation of data (such as words, sentences, or images) that captures its semantic meaning in a high-dimensional space. Embeddings are typically represented as 1-D numpy arrays, i.e. vectors. Each element in the array corresponds to a feature or dimension in the vector space.\n",
    "\n",
    "More on this soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9653770f",
   "metadata": {},
   "source": [
    "## Example Application\n",
    "\n",
    "Below is an example of what we can do with API calls and how we can integrate it into our code. \n",
    "\n",
    "We will build a mini translator using Gemini API!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b72a46",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "For machine translation, we need parallel data. Luckily, we have some datasets available for our use, such as [Helsinki NLP's OPUS MT Datasets](https://github.com/Helsinki-NLP/OPUS-MT-train/blob/master/models/de-en/README.md).\n",
    "\n",
    "I downloaded the [test set](https://object.pouta.csc.fi/OPUS-MT-models/de-en/opus-2019-12-04.test.txt), reformatted it and uploaded to our Data folder as `de-en.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea537032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "translation_set = pd.read_csv(\"../Data/de-en.csv\")\n",
    "translation_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7475e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_set.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc12cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have 5000 examples and we don't need all of them for our example, so let's sample 10 of them\n",
    "\n",
    "sampled_translations = translation_set.sample(n=10, random_state=42).reset_index(drop=True)\n",
    "sampled_translations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e584236",
   "metadata": {},
   "source": [
    "### Evaluations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d2d06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "# https://pypi.org/project/sacrebleu/\n",
    "\n",
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073ac8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "\n",
    "def evaluate_translation(model_translation, references):\n",
    "    \"\"\"\n",
    "    Evaluate translation quality using BLEU and CHRF metrics.\n",
    "    the `model_translation` is a list of translated sentences,\n",
    "    and `references` is a list of lists of reference sentences.\n",
    "    \"\"\"\n",
    "    bleu = sacrebleu.corpus_bleu(model_translation, references)\n",
    "    chrf = sacrebleu.corpus_chrf(model_translation, references)\n",
    "    return bleu, chrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ac2de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(sacrebleu.corpus_bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775667ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(sacrebleu.corpus_chrf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84df2e88",
   "metadata": {},
   "source": [
    "### Gemini Translator\n",
    "\n",
    "I used [this notebook](https://github.com/google-gemini/cookbook/blob/main/examples/Translate_a_Public_Domain_Book.ipynb) as a reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb583dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start building our API client for translation\n",
    "\n",
    "system_instruction = \"\"\"You are a helpful translation assistant. \n",
    "You can translate text from German to English.\n",
    "You will be given a sentence in German, and you should return the translation in English.\n",
    "Do not return any additional text, just the translation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1095c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "safety_settings =[\n",
    "    types.SafetySetting(\n",
    "        category=types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
    "        threshold=types.HarmBlockThreshold.BLOCK_NONE,\n",
    "    ),\n",
    "    types.SafetySetting(\n",
    "        category=types.HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
    "        threshold=types.HarmBlockThreshold.BLOCK_NONE,\n",
    "    ),\n",
    "    types.SafetySetting(\n",
    "        category=types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
    "        threshold=types.HarmBlockThreshold.BLOCK_NONE,\n",
    "    ),\n",
    "    types.SafetySetting(\n",
    "        category=types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
    "        threshold=types.HarmBlockThreshold.BLOCK_NONE,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcb36f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemini_translate(prompt):\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=prompt,\n",
    "        config = types.GenerateContentConfig(\n",
    "            system_instruction=system_instruction,\n",
    "            safety_settings=safety_settings,\n",
    "            temperature=0.1, # we want the model to be deterministic\n",
    "            top_p=0.95,\n",
    "            top_k=20,\n",
    "            candidate_count=1, # we want only one translation\n",
    "            seed=5,\n",
    "        )\n",
    "    )\n",
    "    # Let's see what the whole response looks like\n",
    "    # print(response)\n",
    "    print(response.text)\n",
    "\n",
    "    try:\n",
    "        return response.text\n",
    "    except Exception as ex:\n",
    "        raise ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bedf3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see if our translation function works\n",
    "\n",
    "example = translation_set.iloc[0]\n",
    "print(f\"German: {example['german']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def6c2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gemini_translate(example['german']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5906e6",
   "metadata": {},
   "source": [
    "This is what the whole response looks like:\n",
    "\n",
    "candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, file_data=None, thought_signature=None, code_execution_result=None, executable_code=None, function_call=None, function_response=None, text=\"I think we'd better go now.\")], role='model'), citation_metadata=None, finish_message=None, token_count=None, finish_reason=<FinishReason.STOP: 'STOP'>, url_context_metadata=None, avg_logprobs=None, grounding_metadata=None, index=0, logprobs_result=None, safety_ratings=None)] create_time=None response_id=None model_version='models/gemini-2.5-flash-preview-05-20' prompt_feedback=None usage_metadata=GenerateContentResponseUsageMetadata(cache_tokens_details=None, cached_content_token_count=None, candidates_token_count=9, candidates_tokens_details=None, prompt_token_count=60, prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=60)], thoughts_token_count=163, tool_use_prompt_token_count=None, tool_use_prompt_tokens_details=None, total_token_count=232, traffic_type=None) automatic_function_calling_history=[] parsed=None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f431c6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column for the translations\n",
    "sampled_translations['gemini_translation'] = sampled_translations['german'].apply(gemini_translate)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "sampled_translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635b2472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Gemini translations against the reference translations\n",
    "references = [sampled_translations['english1'].tolist(), sampled_translations['english2'].tolist()]\n",
    "model_translations = sampled_translations['gemini_translation'].tolist()\n",
    "\n",
    "# Initialize lists to store scores\n",
    "bleu_scores = []\n",
    "chrf_scores = []\n",
    "\n",
    "# Loop through each row to evaluate BLEU and CHRF\n",
    "for index, row in sampled_translations.iterrows():\n",
    "    refs = [[row['english1'], row['english2']]]\n",
    "    hyp = [row['gemini_translation']]\n",
    "    bleu, chrf = evaluate_translation(hyp, refs)\n",
    "    bleu_scores.append(round(bleu.score, 2))\n",
    "    chrf_scores.append(round(chrf.score, 2))\n",
    "\n",
    "# Add scores as new columns\n",
    "sampled_translations['bleu_score'] = bleu_scores\n",
    "sampled_translations['chrf_score'] = chrf_scores\n",
    "\n",
    "print(sampled_translations[['german', 'gemini_translation', 'bleu_score', 'chrf_score']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dighum101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
